## 1. 介绍
许多重要的网络分析工作围绕着基于点和边的预测。在经典的节点分类问题中，我们关注的是预测网络节点的可能存在的标签。比如，在社交网络中推测用户爱好，在蛋白质的相互作用网络中进行蛋白质功能预测。类似的，在链接预测中，我们希望预测出网络中的两个节点是都存在边。链接预测应用于多个领域，比如在基因学中帮助发现基因间新的相互作用，在社交网络中辨别真实世界的朋友关系。

任何监督学习机器算法都需要一些能提供信息的，有区别的，独立的特征。在网络预测问题中，这就意味着需要先构造出一些特征向量表示节点和边。一个典型的解决方法就是基于权威知识的手工特征辨别，即使一个折扣所需的繁琐努力特征工程，这种功能通常是专为特定任务并不能概括不同的预测任务。

另一种方法是学习特征表示解决一个优化问题。特征学习的挑战是定义一个目标函数，其中涉及权衡计算效率和预测精度。一方面的频谱，可以直接找到一个功能表示，优化性能的下游预测任务。而这种监督程序结果准确性好，这是由于在需要估计的参数数量放大高训练时间复杂度为代价的。另一方面，目标函数可以被定义为独立的下游预测任务和陈述可以学习在一个纯粹的无监督的方式。这使得优化计算效率和一个精心设计的目标，它会导致任务无关的功能，密切配合任务特定的方法在预测精度。

然而，目前的技术不能令人满意地定义和优化一个合理的目标所需的可扩展的无监督特征学习网络。基于线性和非线性降维方法如主成分分析的经典方法，多维标度及其扩展优化目的，将一个网络等为代表的数据矩阵，它最大限度地提高数据的方差表示。因此，这些方法都涉及到昂贵的大型真实网络适当的数据矩阵的特征分解。此外，将所得的潜申述给差的性能在网络上的各种预测任务。

@@@ 然而当前的技术，即一些经典的基于线性或者非线性的降维技术，比如：Principal
Component Analysis, Multi-Dimensional Scaling 以及基于它们的扩展，通过转换成网络数据矩阵，最大化地降低不一致。因此，这些方法都涉及到特征分解，这在真实世界网络中。而且xxx

因此，我们设计了一个目标算法，旨在保持近邻节点，该算法可通过使用随机梯度下降（SGD）被有效优化。特别地，网络节点可以基于，这样xxx，
图1，观察节点u和s1，他们属于同一个节点区域，但是u和s6则属于两个不同的节点区域，因为他们xxx。
真实世界网络常常。因此有必要让算法能够依靠两个原则进行自学习：@@@

或者，我们可以设计一个目标，旨在保留节点的局部邻域。目标可以有效地优化使用随机梯度下降（SGD）类似于仿真是单隐层的前馈神经网络。最近在这个方向的尝试提出了有效的算法，但依赖于一个刚性的概念的网络邻居，这导致在这些方法主要是不敏感的连接模式独特的网络。具体来说，在网络中的节点可以按照他们属于社区（即同质性）；在其他情况下，组织可以根据网络中节点的结构作用（即结构等价）。例如，在图1中，我们观察到节点U和S1属于相同的紧密编织的节点社区，而节点U和S6在两个不同的社区共享相同的枢纽节点的结构作用。真实世界网络通常表现出这样一个等价的混合物。因此，它允许一个灵活的算法，可以学习的节点表示服从原则本质：学习表示嵌入节点相同的网络社区紧密联系在一起的能力，以及学习表示节点共享相似的角色有类似的嵌入。这将允许功能学习算法概括在各种各样的域和预测任务。

### Present work
我们提出了node2vec, 一种在网络中具有扩展性的半监督学习算法。我们自定义的基于图的优化目标函数使用SGD出于对自然语言处理的前期工作。直观地说，我们的方法返回的特征表示，最大限度地保留在一个d维特征空间节点的网络社区的可能性。我们使用第二阶随机游走的方法来生成（样本）网络节点的街区。

我们的主要做的工作是在定义了一个灵活的概念：网络中一个节点的邻居。通过选择合适的社区的概念，node2vec可以表示组织基于网络角色和/或属于他们的社区节点。我们实现这一点，通过开发一个家庭的偏见随机游动，有效地探索不同的社区的一个给定的节点。该算法是灵活的，通过可调参数控制搜索空间，在以前的工作相比，刚性程序。因此，我们的方法推广之前的工作，可以观察到网络模型等价的全谱。我们的搜索策略的参数有一个直观的解释和偏见走不同的网络勘探策略。这些参数也可以学会直接使用标记数据该式的一小部分。

我们还展示了如何个别节点的特征表示可以扩展到节点对（即边）。为了生成边的特征表示，我们组成的学习特征表示的单个节点使用简单的二进制运算符。这种组合使node2vec预测任务涉及到节点和边。

我们主要关注在网络中两种常见的预测工作，一种是多标签分类，即每个节点被贴上一种或者多种类型标签；另一种时候链接预测，即预测给定两个节点之间是否存在边。我们对比了node2vec算法和state-of-the-art特征学习算法，并且在真实世界网络的多种领域进行实验，比如社交网络，信息网络，以及来自系统生物学的网络。实验表明，node2vec优于state-of-the-art算法，在多标签分类方面高达26.7%，在链接预测方面高达12.6%。该算法拥有很好的表现，即使只有10%的已标签数据，而且在噪点数据或者缺边数据的干扰下很稳定。在计算上，node2vec的重要特点是可并行化，因此可以在短短几小时之间扩展成一个拥有数百万节点的巨大网络。

在整体上，我们这篇论文主要做了下列工作：
- 提出了node2vec，一种针对网络特征学习的高效且可扩展的算法，通过SGD有效优化了网络感知，维护邻域目标。
- 展示了node2vec如何符合网络科学中的既定原则，提供灵活性来发现符合不同等价性的表示。
- 在邻域保护目标的基础上扩展了node2vec和其他特征训练学习方法，从节点到节点对（边）预测任务。
- 依靠真实世界数据集，在多标签分类和链接预测两方面实验性地评估了node2vec算法。

这篇论文的剩余部分组织如下：
- 第二部分，简单地调查了和网络特征学习的相关工作。
- 第三部分，利用node2vec，介绍了网络特征学习技术细节。
- 第四部分，实验性地评估了node2vec预测任务在现实世界中的各种网络的节点和边和评估参数的灵敏度，扰动分析，可扩展性分析。
- 第五部分，总结了node2vec框架，并强调说明了在未来有希望的一些发展方向。
node2vec的实验数据和参考实现见：http://snap.stanford.edu/node2vec

## 2. 相关工作
特征工程已被广泛研究的机器学习社区的各种标题下。在网络中，传统的范例产生功能的节点是基于特征提取技术，通常涉及一些种子手工制作的功能，基于网络属性。相比之下，我们的目标是自动化的整个过程铸造特征提取为代表的学习问题，在这种情况下，我们不需要任何手工设计的功能。

无监督特征学习方法通常利用图的各种矩阵表示，特别是拉普拉斯算子和邻接矩阵的谱特性。在这个线性代数的角度来看，这些方法可以被看作是降维技术。几种线性（例如，PCA）和非线性（如IsoMap）降维技术已经提出了。这些方法遭受计算和统计性能的缺点。在计算效率方面，一个数据矩阵的特征值分解是昂贵的解决方案，除非质量显着影响的近似，因此，这些方法很难适应大规模网络。其次，优化目标是不是观察网络的不同模式强大的这些方法（如同质性和结构等价）和对底层网络结构和预测任务之间的关系假设。例如，谱聚类具有很强的同质性假设图削减将是有用的分类。这样的假设是合理的，在许多情况下，但不能有效地概括不同的网络。

自然语言处理的代表性学习的最新进展开辟了新的方法，如单词的离散对象的特征学习。特别是，跳过克模型的目的是学习连续的特征表示的话，通过优化邻域保留似然目标。该算法的收益如下：它扫描的话，一个文件，并为每一个字，它的目的是嵌入这样的话的功能可以预测附近的话（即，在一些上下文窗口的话）。词的特征表示的优化的可能性目的使用SGD负采样了。跳过克目标是基于分布假设，认为在类似的上下文中的词语往往有类似的含义。也就是说，类似的词往往出现在类似的词社区。

受到Skip-gram模式[1]的启发，最近的研究通过将网络表示为“文档”，建立了网络的类比。与文档相同的方式是单词的有序序列，可以从底层网络中采样节点序列，并将网络转换为有序序列的节点。然而，节点有许多可能的采样策略，导致不同的学习特征表示。事实上，正如我们将要展示的，没有明确的获胜抽样策略可以在所有网络和所有预测任务中运行。这是以前工作的一个主要缺点，它不能在网络中对节点进行抽样提供任何灵活性。我们的算法node2vec通过设计一个不受特定采样策略束缚的灵活目标并提供参数来调整探索的搜索空间，从而克服了这一限制（见第3节）。

最后，对于基于节点和边的预测任务，基于现有和新颖的图形特定深层网络架构，有一个最新的监督功能学习工作。这些架构直接使用几层非线性变换直接最小化下游预测任务的损失函数，这导致高精度，但是由于训练时间要求高，以可扩展性为代价。

## 3. 特征学习框架
我们将网络中的特征学习制定为最大似然优化问题。 令G =（V, E）为给定网络。 ur分析是一般的，适用于任何（un）有向，（un）加权网络。 让f：V！ Rd是从节点到特征表示的映射函数，我们旨在学习下游预测任务。 这里d是指定我们的特征表示的维数的参数。 等价地，f是大小为|V| x d的矩阵。 对于每个源节点u∈V，我们定义NS(u)∈ V作为通过邻域采样策略S生成的节点u的网络邻域。

我们继续将Skip-gram架构扩展到网络。我们寻求优化以下目标函数，从而最大化观察网络邻域NS(u)的对数概率

基于Skip-gram架构的特征学习方法最初是在自然语言的背景下开发的。 给定文本的线性，邻域的概念可以使用连续词上的滑动窗口来自然地定义。 然而，网络不是线性的，因此需要一个更加丰富的社区概念。 为了解决这个问题，我们提出了一个随机化的过程，它对给定源节点u的许多不同邻域进行采样。 NS（u）不仅仅是直接邻居，而是依赖于采样策略S而具有非常不同的结构。

### 3.1 经典搜索策略
我们把对一个原点的近邻节点取样看成一种局部搜索的形式。图1表明，给定一个点原点u，我们的目标是取样得到u的邻居节点集Ns(u)，更重要的是，为了能更公平地比较不同取样策略S，我们约定近邻居节点集合的大小为k，然后对u点多次取样。总的来说，针对前面所说的，即取样得到邻居节点集Ns其中的k个节点，有两种完全不同的取样策略：
	- 广度优先取样(BFS)：邻居节点集被限制于和原点直接相连的节点周围。比如图1，当k=3时，取得的节点集为s1，s2，s3
	- 深度优先取样（DFS）：邻居节点集由离原点距离递增的顺序节点组成。比如图1，当k=3时，取得的节点集为s4，s5，s6

广度优先和深度优先取样代表极端的情况下，他们探索的搜索空间不同，导致不同的意义。

特别地，基于节点的网络预测工作经常在两种相似性上变换：同质等价性和结构等价性。在同质等价性中，节点间紧密互联，并且属于同一个相似网络区域，应该被聚集在在一起（例如图1中节点s1和节点u属于同一个网络区域）。对照地，在结构等价性中，网络中拥有相似结构的节点应该被聚集在一起（例如图1中节点u和节点s6分别聚集着对应的网络区域）。更重要的是，不同于同质性，结构等价性不看重相连性，节点间可能相距很远但是仍然拥有相似的结构特征。在真实世界中，这些等价概念并不互斥，网络经常表现为某些节点间存在同质等价性，某些节点间存在结构等价性。

我们观察到，BFS和DFS策略在产生反映上述等价物的表示中起着关键作用。特别地，通过BFS策略近邻取样，更加符合结构等价性。直观上，我们可以看到为了确定结构等价性，通常足以准确地表征当地社区。举个例子，结构等价性基于网络节点，比如桥梁和集线器等。例如，通过将搜索限制到附近的节点，BFS实现了这种表征，并获得了每个节点邻域的微观视图。此外，在BFS中，采样邻域中的节点往往重复多次。 这也是重要的，因为它减少了表征相对于源节点的1跳节点的分布的方差。然而，对于任何给定的k，这仅仅探索了是图中非常小的一部分。

与DFS相反，DFS可以探索网络的较大部分，因为它可以远离源节点u移动（样本大小k固定）。 在DFS中，采样节点更准确地反映了邻域的宏观视图，这是基于同质性推断社区所必需的。 然而，DFS的问题在于，不仅要推断网络中存在哪些节点到节点的依赖关系，还要表征这些依赖关系的确切性质。 这是很困难的，因为我们对样本量有一个限制，一个大的社区要探索，导致高差异。 其次，移动到更深的深度导致复杂的依赖性，因为采样节点可能远离源并且可能不太具有代表性。


### 3.2 node2vec
在上述基础上，我们设计了一种灵活的且能够在BFS和DFS之间平滑插值的邻居取样策略。我们通过偏随机游走程序对其做了实现。

#### 3.2.1 随机游走
正式地，给定一个节点u，固定长度l，模拟随机游走，ci表示走到的第i个节点，起点c0为u。ci用下面的方法生成：

$$ P\left ( c_{i} = x | c_{i - 1} = v\right ) = \left\{\begin{matrix}
 & \frac{\pi_{vx}}{Z} & if (u, x) \in Z \\ 
 & 0 & otherwise
\end{matrix}\right. $$

π(vx)是一个非规范化转移概率，Z是一个常量。

#### 3.2.2 搜索偏量α

最简单的偏置随机游走的方法是基于v和x之间的静态权值w(vx)，π(vx)=w(vx)（对于无权图相当于w(vx)=1）。但是，这种方式不能说明网络结构，不能引导搜索程序去探索不同的网络。 另外，不同于BFS和DFS，它们分别适用于结构等同性和同质性的极端采样范例，我们的随机游走应适应这些事实，即这些等价概念不是竞争或排斥的，而现实世界的网络通常表现出两者的混合。

我们定义了两阶随机游走参数p和q来引导游走：在图2中，假设随机游走刚刚经过了边(t,v)，现在驻留在节点v处，现在需要决定下一步怎么走。我们规定非标准化过渡概率π(v,x)=αpq(t, x) * · w(vx)，

$$ a_{pq} (t, x) = \left\{\begin{matrix}
 \frac{1}{p} & if \ d_{tx} = 0 \\ 
 1 & if \ d_{tx} = 1 \\ 
 \frac{1}{q} & if \ d_{tx} = 2 \\ 
\end{matrix}\right. $$

$ d_{tx} $表示t和x之间的最短路径，注意其值必定属于{0,1,2}。参数p和q是引导游走的必要条件。

直观地，参数p和q控制着游走速度，且能够让搜索程序在BFS和DFS策略间切换，从而反映出不能概念的节点等价性。

Return-parameter p，参数p控制着游走过程中重复访问节点的可能性。一方面，当设置$ p > max(q, 1) $时，游走过程中的接下来两步重复访问节点的概率变小（除非当前节点没有其他相邻节点了）。这种策略很好地促进了节点探索的稳定性，而且避免了接下来两步重复取样。另一方面，当设置 $ p < min(q, 1) $时，则更趋向于回溯游走，这种策略使得游走保持在原点u周围。

In-Out parameter, 参数q控制着游走向内还是向外。回到图2中，如果 $ q > 1 $，则随机游走更偏向于走到节点t周围的节点。这样我们就可以得到节点周围的节点，近似于BFS策略。相反地，如果 $ q < 1 $，则游走更偏向于走到离节点t更远的节点，则反映出DFS策略中更加倾向于向外部探索。但是，一个非常重要的差别是我们是在随机游走的大框架下实现的类DFS探索策略，因此，取样节点序列与原点u的距离并不是严格递增的。反过来，而是从易处理的预处理和随机散射的优良采样效率中获益。注意通过设置π(vx)为前置节点t的一个函数表示，则随机游走是二阶马尔科夫。


注：$ q > 1 $，则 $ 1 / q < 1 == d_{tx}=1 $； $ q < 1 $, 则$ 1 / q > 1 == d_{tx}=1 $

Benefits random walks。相对于纯BFS/DFS策略，这种方法更有优势：就相同的空间和时间限制下，随机游走计算效率更高。存储与每个节点直接相连节点的空间复杂度为O(|E|)。对于二阶随机游走，存储每个节点的邻居之间的互连是有帮助的, 导致了空间复杂度为O(a^2 * v), a为图的平均度数，在真实世界网络中，通常比较小。随机游走相对于传统搜索取样策略，另外一个关键的优势在于时间复杂度更低。特别地，随机游走提供了更加方便的机制，通过在不同的源点上重复使用样本，从而提有效高取样效率。通过模拟长度l > k 的随机游走，由于随机游走的Markovian性质，我们可以一次生成l个k个节点的k个样本。每个样本的时间复杂度为O（l / k * (l - k)）。举个例子，图1中，随机游走样本为 {u, s4, s5, s6, s8, s9}，l = 6，则Ns(u) = {s4, s5, s6}，NS(s4) = {s5, s6, s8} and NS(s5) =
{s6, s8, s9}. 注意样本重复使用可能引入一些偏差，但是，我们观察到它大大提高了效率。

伪代码：

node2vec的伪代码在算法1中给出。在任意随机游走中，由于选择起始节点u，存在隐含的偏差。 由于我们学习了所有节点的表示，我们通过模拟从每个节点开始的固定长度l的随机散列来抵消这种偏差。 在步行的每个步骤，基于转移概率πνx进行抽样。 可以预先计算二阶马尔科夫链的转移概率πνx，因此在使用别名抽样的O（1）时间内可以有效地进行模拟随机游走的节点采样。node2vec的三个阶段，即用于计算转移概率的预处理，随机游走模拟和使用SGD的优化顺序执行。每个阶段可以并行化并且异步执行，有助于node2vec的整体可扩展性。
node2vec可在这里获得: http://snap.stanford.edu/node2vec


### 3.3 边特征学习
node2vec算法提供了一种半监督学习方法，从而学习到网络中节点的更多的特征表示。但是，人们感兴趣的是围绕节点与节点对的预测，而不是独立的节点，例如，在链接预测中，我们预测两个节点间是否存在边。由于我们的随机游走自然是基于底层网络中的节点之间的连接结构，所以我们使用引导方法在单个节点的特征表示上将它们扩展到对节点。

给定两个节点u和v，我们定义一个二进制运算符O以便生成表示g(u,v)，使得g：V'* V -> Rd'，其中d'是对（u，v）的表示尺寸）。我们希望我们的运算符通常被定义为任何一对节点，即使该对之间不存在边缘，因为这样做使得表示对链接预测有用，其中我们的测试集包含真和假边（即不存在）。我们考虑运营商的几个选择？使得d'= d，其总结在表1中。


## 4. 实验

### 4.1  案例学习
在3.1节中，我们已经了解到BFS和DFS策略代表不同的xxx在同质等价性和结构等价性，现在我们需要用实验证明node2vec能够发现聚集并且遵循两个等价性原则。

我们需要用到的网络是符合xxx特征。网络中有77个节点和254条边，设置d=16，运行node2vec算法对图中每个节点去进行特征学习。特征表示通过k-means算法聚集。然后可视化原来的网络xxx

图3上半部分展示的是当p=1, q=0.5时的结果。xxx用相同的颜色表示。在这种设置下，集群区域两个。特征内部之间的边，我们可以得出这种特征相近 同质等价性。

为了探索有多少节点有相同的结构作用，我们用相同的图，只不过设置$ p=1, q=2$，使用node2vec获取节点特征，然后根据特征将节点聚集。xxx，图3下半部分，例如，node2vec聚集蓝色节点，这些节点表现出的特征为连接小说中的次要情节。相似地，黄色节点在外围且xxx. 通过实验表明，xxx


### 4.2

我们的实验评估了通过node2vec获得的标准监督学习任务的特征表示：节点的多标签分类和边缘的链接预测。对于这两个任务，我们根据以下特征学习算法评估node2vec的性能：
光谱聚类[29]：这是一个矩阵因式分解方法，其中我们将图G的归一化拉普拉斯矩阵的顶点特征向量作为节点的特征向量表示。
DeepWalk [24]：这种方法通过模拟均匀随机游走来学习维度特征表征。 DeepWalk中的采样策略可以看作是p = 1和q = 1的node2vec的一个特殊情况。
LINE [28]：这种方法在两个单独的阶段中学习d维特征表示。在第一阶段，它通过BFS风格的模拟学习d = 2维度，直接在节点的近邻上。在第二阶段，它通过从源节点严格地以2跳距离采样节点来学习下一个d = 2维度。


在3.1节中，我们观察到BFS和DFS策略是基于同质性（即网络社区）和结构等价性（即节点的结构角色）原理的嵌入节点频谱的极端结果。我们现在的目的是经验性地证明这一事实，并表明node2vec实际上可以发现遵守这两个原则的嵌入。我们使用一个网络，其中节点对应于小说“LesMisérables”[13]中的字符，边缘连接共同的字符。网络有77个节点和254个边缘。我们设置d = 16并运行node2vec来学习网络中每个节点的特征表示。特征表示使用k-means进行聚类。然后，我们可以将二维的原始网络可视化，其中节点现在根据其集群分配颜色。图3（上图）显示了当我们设置p = 1，q = 0.5时的示例。请注意网络区域（即网络社区）是如何使用相同的颜色进行着色的。在此设置中，node2vec发现小说主要子图中频繁交互的人物群集/群体。由于角色之间的边缘是基于共同的，我们可以得出结论，这种表征与同情密切相关。

为了发现哪些节点具有相同的结构角色，我们使用相同的网络，但设置p = 1，q = 2，使用node2vec获取节点特征，然后根据获得的特征对节点进行聚类。这里node2vec获得节点到集群的补充分配，使得颜色对应于结构等同，如图3（底部）所示。例如，node2vec将蓝色节点靠近在一起嵌入。这些节点表示作为小说不同子图之间桥梁的字符。类似地，黄色节点主要表示在边缘处具有有限的相互作用的字符。人们可以为这些节点集群分配替代语义解释，但是关键的部分是node2vec与特定的等价概念无关。正如我们通过实验展示的，这些等价概念通常在大多数现实世界网络中展现，并且对于预测任务的学习表示的性能具有显着影响。


我们排除了已经被证明不如DeepWalk的其他矩阵分解方法[24]。我们也排除最近的一种方法，GraRep [6]，将LINE概括为将网络社区的信息整合到2跳以上，但无法有效地扩展到大型网络。与先前工作中用于评估基于抽样的特征学习算法的设置相反，我们为每个方法生成相等数量的样本，然后评估所获得的特征在预测任务上的质量。在这样做的时候，我们仅仅因为执行语言（C / C ++ / Python）而观察到的性能增益，因为它是次要的算法。因此，在采样阶段，设置DeepWalk，LINE和node2vec的参数，使得它们在运行时生成相等数量的采样。例如，如果K是整体抽样预算，那么node2vec参数满足K = r？ *？ l * | V |。在优化阶段，所有这些基准优化使用SGD与我们纠正的两个关键差异。首先，DeepWalk使用分层采样来近似softmax概率，其目的与node2vec所使用的目标类似。然而，与负采样相比，分层softmax是无效的。因此，保持一切都一样，我们切换到DeepWalk中的负采样，这也是node2vec和LINE中的事实上的近似。第二，node2vec和DeepWalk都有一个参数，用于优化上下文邻居节点的数量，数量越多，需要更多轮优化。对于LINE，此参数设置为1，但是由于LINE比其他方法更快地完成单个历元，所以我们让它运行k个时期。
用于node2vec的参数设置符合DeepWalk和LINE的典型值。具体来说，我们设置d = 128，r = 10，l = 80，k = 10，并且对于单个时期运行优化。我们重复我们的10次随机种子初始化的实验，我们的结果在p值小于0.01时具有统计学意义。使用10倍标记数据的10倍交叉验证了解最佳输入和返回超参数网格搜索p，q {0.25，0.50，1，2，4}。


### 4.4 参数分析
node2vec算法涉及多个参数，在图5a中，我们检查参数的不同选择如何影响BlogCatalog数据集上的node2vec的性能，使用标记和未标记数据之间的50-50分割。除了正在测试的参数外，所有其他参数均为​​默认值。 p和q的默认值设置为1。我们测量宏观F1分数作为参数p和q的函数。 node2vec的性能随着输入参数p和返回参数q的减小而提高。这种性能的提高可以基于我们期望在BlogCatalog中看到的同质性和结构性等同性。虽然低q鼓励外部探索，但是由低p平衡，确保步行距离起始节点不会太远。我们还研究了特征数d和节点邻域参数（步数r，步长l和邻域大小k）如何影响性能。我们观察到，一旦表示的尺寸达到100，性能就会饱和。同样，我们观察到，增加每个源的行走数量和长度可以提高性能。
这并不奇怪，因为我们有更大的总体抽样预算K来学习表示。这两个参数对该方法的性能有较高的影响。有趣的是，上下文大小k也以提高的优化时间为代价来提高性能。然而，在这种情况下，性能差异并不大。


## 5. 讨论总结

在本文中，我们研究了网络中的特征学习作为搜索式优化问题。 这个观点给了我们多种优势。 它可以在探索 - 开发权衡的基础上解释经典搜索策略。 另外，当应用于预测任务时，它为所学习的表示提供了一定程度的可解释性。 例如，我们观察到BFS只能探索有限的社区。 这使得BFS适合于表征依赖于节点立即本地结构的网络中的结构等价性。 另一方面，DFS可以自由地探索以高差异代价发现同源社区的重要环节。

DeepWalk和LINE都可以被视为网络上的严格搜索策略。 DeepWalk [24]提出使用均匀随机游走进行搜索。 这样一个策略的明显局限在于它使我们无法控制被探查的社区。 LINE [28]主要提出了一个宽度优先策略，采样节点和仅在1跳和2跳邻居上独立地优化似然。 这种探索的效果更容易表征，但是它是限制性的，并且在进一步深度探索节点时没有灵活性。 相比之下，node2vec中的搜索策略既灵活又可控，通过参数p和q探索网络邻域。 虽然这些搜索参数具有直观的解释，但是当我们可以直接从数据中学习时，我们在复杂网络上获得最佳结果。 从实际的角度来看，node2vec是可扩展的，并且对于扰动是鲁棒的。

我们展示了节点嵌入对链接预测的扩展性能优于专为此任务设计的流行启发式分数。 我们的方法允许超出表1中列出的其他二进制运算符。作为未来的工作，我们想探索Hadamard运算符相对于其他运算符的成功背后的原因，以及根据搜索参数确定边缘的可解释等价概念。 node2vec的未来扩展可能涉及具有特殊结构的网络，例如异构信息网络，具有用于节点和边缘的显式域特征的网络和有符号边缘网络。 连续特征表示是许多深度学习算法的支柱，使用node2vec表示作为图形端对端深度学习的构建块是非常有趣的。







原文链接：http://www.cs.stanford.edu/people/jure/pubs/node2vec-kdd16.pdf
